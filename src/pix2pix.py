import pytorch_lightning as pl
import torch
import torch.nn.functional as F
import torch.optim as o
import torch.nn as nn

from torchmetrics.image import SpectralAngleMapper
from torchmetrics.functional import peak_signal_noise_ratio, accuracy, structural_similarity_index_measure

import config as c
import src.utils as u
from src.generator_model import Generator
from src.discriminator_model import Discriminator

class Pix2Pix(pl.LightningModule):
    def __init__(self, run):
        super(Pix2Pix, self).__init__()
        # Important to disable automatic optimization as it 
        # will be done manually as there are two optimizators
        self.automatic_optimization = False
        self.generator_lr = c.LEARNING_RATE               # Generator learning rate
        self.discriminator_lr = c.LEARNING_RATE       # Discriminator learning rate

        # Models
        self.discriminator = Discriminator(in_channels_x=c.SHAPE_X[0], in_channels_y=c.SHAPE_Y[0]).to(c.DEVICE)
        self.generator = Generator(in_channels=c.SHAPE_X[0], out_channels=c.SHAPE_Y[0], features=64).to(c.DEVICE)
        # Optimizer and Scheduler
        # opt_disc = o.Adam(self.discriminator.parameters(), lr=c.LEARNING_RATE, betas=(0.5, 0.999))
        # sched_disc = o.lr_scheduler.ExponentialLR(self.opt_disc, gamma=1.05)
        # opt_gen = o.Adam(self.generator.parameters(), lr=c.LEARNING_RATE, betas=(0.5, 0.999))
        # sched_gen = o.lr_scheduler.ExponentialLR(self.opt_disc, gamma=1.05)
        # self.optimizers = (opt_gen, opt_disc)
        # self.lr_schedulers = (sched_gen, sched_disc)
        # Loss Functions
        self.BCE = nn.BCEWithLogitsLoss()
        self.L1_LOSS = nn.L1Loss()
        self.SPECTRAL_LOSS = SpectralAngleMapper().to(c.DEVICE)

        # Neptune run
        self.run = run

    def forward(self, x):
        return self.generator(x)
    
    def generator_loss(self, prediction_image, target_image, prediction_label, target_label):
        """
        Generator loss (a combination of): 
            1 - Binary Cross-Entropy
                Between predicted labels (generated by the discriminator) and target labels which is all 1s
            2 - L1 / Mean Absolute Error (weighted by lambda)
                Between generated image and target image
            3 - Spectral Loss (Spectral Angle)
                Between generated image and target image
            4 - LFM Loss (Feature Matching Loss)
                Between weights from all layers of the discriminator
        """
        # Adverserial loss
        BCE_loss = self.BCE(prediction_label[0], torch.ones_like(target_label)) * c.ADV_LAMDA
        # LFM loss
        LFM_loss = torch.zeros((1,1,1,1)).to(c.DEVICE)
        # LFM_weights = [1./16, 1./8, 1./4, 1./4, 1./2, 1.]
        # for i, tensors in enumerate(zip(prediction_label[1], prediction_image[1])):
        #     LFM_loss += self.L1_LOSS(tensors[0], tensors[1]) * LFM_weights[i] * c.LFM_LAMBDA
        # Other losses
        L1 = self.L1_LOSS(prediction_image, target_image) * c.L1_LAMBDA
        SPEC = self.SPECTRAL_LOSS(prediction_image, target_image) * c.SPEC_LAMBDA

        return BCE_loss, L1, SPEC, LFM_loss
    
    def discriminator_loss(self, prediction_label, target_label):
        """
        Discriminator loss: 
            1 - Binary Cross-Entropy
                Between predicted labels (generated by the discriminator) and target labels
                The target would be all 0s if the input of the discriminator is the generated image (generator)
                The target would be all 1s if the input of the discriminator is the target image (dataloader)
        """
        bce_loss = self.BCE(prediction_label, target_label) # index 0 since these are the output weights
        return bce_loss
    
    def configure_optimizers(self):
        """
        Using Adam optimizer for both generator and discriminator
        Both would have different initial learning rates
        Stochastic Gradient Descent with Warm Restarts is also added as learning scheduler (https://arxiv.org/abs/1608.03983)
        """
        # Optimizers
        generator_optimizer = torch.optim.Adam(self.generator.parameters(), lr=self.generator_lr, weight_decay=1e-5)
        discriminator_optimizer = torch.optim.Adam(self.discriminator.parameters(), lr=self.discriminator_lr, weight_decay=1e-5)
        # Learning Scheduler
        generator_lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(generator_optimizer, T_0=1000, T_mult=2)
        discriminator_lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(discriminator_optimizer, T_0=1000, T_mult=2)
        return [generator_optimizer, discriminator_optimizer], [generator_lr_scheduler, discriminator_lr_scheduler]

    def training_step(self, batch, batch_idx):
        # Optimizers
        generator_optimizer, discriminator_optimizer = self.optimizers()
        generator_lr_scheduler, discriminator_lr_scheduler = self.lr_schedulers()
        
        image, target = batch
        image_i, image_j = (image, image) #torch.split(image, c.BATCH_SIZE)
        target_i, target_j = (target, target) #torch.split(target, c.BATCH_SIZE)
        # Should Disc and Gen be trained on different images?
        
        ######################################
        #  Discriminator Loss and Optimizer  #
        ######################################
        # Generator Feed-Forward
        generator_prediction = self.forward(image_i)
        generator_prediction = torch.clip(generator_prediction, 0, 1)
        # Discriminator Feed-Forward
        discriminator_prediction_real = self.discriminator(image_i, target_i)
        discriminator_prediction_fake = self.discriminator(image_i, generator_prediction)
        # Discriminator Loss
        discriminator_label_real = self.discriminator_loss(discriminator_prediction_real[0], 
                                                           torch.ones_like(discriminator_prediction_real[0]))
        discriminator_label_fake = self.discriminator_loss(discriminator_prediction_fake[0],
                                                           torch.zeros_like(discriminator_prediction_fake[0]))
        discriminator_loss = discriminator_label_real + discriminator_label_fake
        # Discriminator Optimizer
        discriminator_optimizer.zero_grad()
        discriminator_loss.backward()
        discriminator_optimizer.step()
        discriminator_lr_scheduler.step()
        
        ##################################
        #  Generator Loss and Optimizer  #
        ##################################
        #  Generator Feed-Forward
        generator_prediction = self.forward(image_j)
        generator_prediction = torch.clip(generator_prediction, 0, 1)
        # Discriminator Feed-Forward
        discriminator_prediction_fake = self.discriminator(image_j, generator_prediction)
        # Generator loss
        generator_bce_loss, generator_l1_loss, generator_spec_loss, generator_lfm_loss = self.generator_loss(generator_prediction, target_j,
                                                                                        discriminator_prediction_fake,
                                                                                        torch.ones_like(discriminator_prediction_fake[0]))
        generator_loss = (generator_bce_loss * c.ADV_LAMDA) + (generator_l1_loss * c.L1_LAMBDA) + (generator_spec_loss * c.SPEC_LAMBDA) + (generator_lfm_loss * c.LFM_LAMBDA)
        # Generator Optimizer
        generator_optimizer.zero_grad()
        generator_loss.backward()
        generator_optimizer.step()
        generator_lr_scheduler.step()
        
        # Progressbar and Logging
        current_loss = {}
        current_loss["gen/loss"] = generator_loss.item()
        current_loss["dis/loss"] = discriminator_loss.item()
        current_loss['gen/l1_loss'] = generator_l1_loss.item()
        current_loss['gen/gan_loss'] = generator_bce_loss.item()
        current_loss['gen/spec_loss'] = generator_spec_loss.item()
        current_loss['gen/lfm_loss'] = generator_lfm_loss.item()
        current_loss['dis/label_real'] = discriminator_label_real.item()
        current_loss['dis/label_fake'] = discriminator_label_fake.item()
        current_loss['gen/lr'] = generator_lr_scheduler.get_last_lr()[0]
        current_loss['dis/lr'] = discriminator_lr_scheduler.get_last_lr()[0]
        self.log_loss(current_loss)

        return current_loss
    
    def log_loss(self, loss):
        # Neptune log
        for key, value in loss.items():
            self.run[key].log(value)

    def validation_step(self, batch, batch_idx):
        image, target = batch
        
        # Generator Feed-Forward
        generator_prediction = self.forward(image)
        generator_prediction = torch.clip(generator_prediction, 0, 1)
        # Generator Metrics
        generator_psnr = peak_signal_noise_ratio(generator_prediction, target)
        generator_ssim = structural_similarity_index_measure(generator_prediction, target)
        discriminator_prediction_fake = self.discriminator(image, generator_prediction)
        generator_accuracy = accuracy(discriminator_prediction_fake[0], torch.ones_like(discriminator_prediction_fake[0], dtype=torch.int32), task='binary')
        
        # Discriminator Feed-Forward
        discriminator_prediction_real = self.discriminator(image, target)
        discriminator_prediction_fake = self.discriminator(image, generator_prediction)
        # Discriminator Metrics
        discriminator_accuracy = accuracy(discriminator_prediction_real[0], torch.ones_like(discriminator_prediction_real[0], dtype=torch.int32), task='binary') * 0.5 + \
                                accuracy(discriminator_prediction_fake[0], torch.zeros_like(discriminator_prediction_fake[0], dtype=torch.int32), task='binary') * 0.5
            
        # Progressbar and Logging
        metrics = {'val/gen/psnr': generator_psnr, 'val/gen/ssim': generator_ssim, 
                   'val/gen/accuracy': generator_accuracy, 'val/dis/accuracy': discriminator_accuracy}
        for key, value in metrics.items():
            self.run[key].log(value)
        return metrics


if __name__ == "__main__":
    pass